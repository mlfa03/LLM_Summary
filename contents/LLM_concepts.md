<p id="reading-time-action-id" align="left">1 min read</p>

[**← Back to contents**](../common/contents.md)

# LLM Concepts

**Context Window** 
* Maximum size of the context that we can give to an
LLM

**Prompts**
* Context given to the LLM

**Prompt engineering**
* Techniques to build a prompt
* It is an iterative process

Risks associated with prompts:
* Hallucination - when the LLM gives a fake response and it seems true. 
* Prompt injection
* Prompt leaking - when LLM model give you sensitive, private or
confidential information.
* Jailbreaking - Form of prompting injection designed to bypass the safety and moderation
features of an LLM model.

Because of this risk, most companies:
○ do not want to build in a public cloud, but behind some kind of security or wall.
○ do not want to send info to chatGPT because they are not sure about what is OpenAI going to
do with their data.
## The End

[**Continue to Contributions →**](../common/contributions.md)
